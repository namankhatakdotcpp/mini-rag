# ğŸš€ Mini RAG â€” Production-Grade Retrieval Augmented Generation System

A full-stack **production-style Retrieval-Augmented Generation (RAG)** system that enables grounded question answering over custom documents using modern LLM infrastructure.

This project demonstrates how real-world AI systems like **ChatGPT Retrieval, Perplexity AI, and enterprise copilots** are built.

---

# ğŸ§  System Overview

Traditional LLMs hallucinate.
This system eliminates hallucinations by retrieving real context before generating answers.

### Pipeline

```
User Query
   â†“
Embedding (Gemini)
   â†“
Vector Search (Pinecone)
   â†“
Reranking (Cohere)
   â†“
Grounded Generation (Gemini)
   â†“
Answer + Sources
```

---

# âœ¨ Features

### ğŸ”¹ Document Ingestion

* Upload text or files
* Smart chunking with overlap
* Metadata tracking for sources

### ğŸ”¹ Vector Retrieval

* Semantic search via Pinecone
* Cosine similarity matching
* Top-K context retrieval

### ğŸ”¹ Reranking Layer

* Cohere rerank model
* Improves relevance before generation
* Production-style search pipeline

### ğŸ”¹ Grounded Answer Generation

* Gemini LLM
* Uses retrieved context only
* Prevents hallucinations
* Source-aware answers

### ğŸ”¹ Full Stack System

* FastAPI backend
* Interactive frontend console
* Real-time ingestion and querying
* Deploy-ready architecture

---

# ğŸ—ï¸ Tech Stack

## AI/ML

* Gemini API (LLM + embeddings)
* Pinecone (vector database)
* Cohere (reranking)

## Backend

* FastAPI
* Python

## Frontend

* HTML/CSS/JS

## Deployment

* Render (backend)
* Vercel (frontend)
* GitHub

---

# âš™ï¸ Architecture (Production Style)

## Ingestion Pipeline

1. Document â†’ chunking
2. Chunk â†’ embedding
3. Embedding â†’ vector DB
4. Metadata stored with vectors

## Query Pipeline

1. Query â†’ embedding
2. Vector search (top-k)
3. Cohere rerank
4. Context construction
5. LLM generation

## Grounded Response

* Uses retrieved context only
* Returns answer with sources
* Prevents hallucinations

---

# ğŸ§ª Example Use Cases

* Chat with PDFs
* AI research assistant
* Internal company knowledge bot
* Customer support AI
* Documentation search engine

---

# ğŸ“Š Why This Project Matters

Most AI demos = simple chatbot calls.

This project replicates **real-world LLM infrastructure**:

| Feature                 | Basic Chatbot | This Project |
| ----------------------- | ------------- | ------------ |
| Vector search           | âŒ             | âœ…            |
| RAG pipeline            | âŒ             | âœ…            |
| Reranking               | âŒ             | âœ…            |
| Grounded answers        | âŒ             | âœ…            |
| Production architecture | âŒ             | âœ…            |

Used in systems like:

* OpenAI retrieval stack
* Perplexity AI
* Google Gemini search
* Enterprise AI copilots

---

# ğŸ§‘â€ğŸ’» How to Run Locally

### Clone repo

```
git clone https://github.com/namankhatakdotcpp/mini-rag
cd mini-rag
```

### Create environment

```
python -m venv venv
source venv/bin/activate
```

### Install dependencies

```
pip install -r requirements.txt
```

### Add API keys (.env)

```
GEMINI_API_KEY=
PINECONE_API_KEY=
COHERE_API_KEY=
PINECONE_INDEX=mini-rag
```

### Run server

```
python main.py
```

Open browser:

```
http://localhost:8000
```

---

# ğŸ§  Engineering Highlights

* Retrieval-first architecture
* Modular RAG pipeline
* Grounded answer generation
* Vector DB integration
* Production-style design

---

# ğŸ† Resume Value

Demonstrates:

* Applied LLM engineering
* Vector databases
* RAG architecture
* Full-stack AI system design
* Production deployment thinking

---

# ğŸ‘¨â€ğŸ’» Author

**Naman**
AI/ML Engineer | Full Stack Developer

Built to demonstrate real-world LLM infrastructure and production RAG pipelines.
